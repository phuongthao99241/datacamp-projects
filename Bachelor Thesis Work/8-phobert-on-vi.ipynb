{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Application of PhoBERT (Nguyen and Nguyen, 2020) on Vietnamese Dataset\n",
    "\n",
    "source: https://github.com/VinAIResearch/PhoBERT"
   ],
   "metadata": {
    "id": "Q4zfujQVnUS-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Install necessary libraries for script"
   ],
   "metadata": {
    "id": "F6jy0Tw7n_sH"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dcV8cInRqvO4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5baa836a-2187-4be3-c072-7824cca98ac4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X3qUsdzeqsag",
    "outputId": "4d0730bf-4986-43f8-906f-6337fdae8877"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mCcUSAmUq33h",
    "outputId": "6639e252-8788-4f84-b633-f7b94afc2399"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/43.6 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━\u001B[0m \u001B[32m41.0/43.6 kB\u001B[0m \u001B[31m1.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m43.6/43.6 kB\u001B[0m \u001B[31m933.1 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.23.5)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=f9922f2490abd2471bea345429a21f12db68dbcd79fb8fdb113b5e2a720d339d\n",
      "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-1.2.2\n"
     ]
    }
   ],
   "source": [
    "pip install seqeval"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Running it is necessarcy to use FastTokenizer"
   ],
   "metadata": {
    "id": "jhtGiasLoMuv"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hXVq6y89rDh4",
    "outputId": "7240a49e-a721-45b2-d6a9-4b2b993c5f80"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into 'transformers'...\n",
      "remote: Enumerating objects: 138580, done.\u001B[K\n",
      "remote: Counting objects: 100% (47/47), done.\u001B[K\n",
      "remote: Compressing objects: 100% (46/46), done.\u001B[K\n",
      "remote: Total 138580 (delta 1), reused 47 (delta 1), pack-reused 138533\u001B[K\n",
      "Receiving objects: 100% (138580/138580), 145.02 MiB | 14.87 MiB/s, done.\n",
      "Resolving deltas: 100% (104140/104140), done.\n",
      "/content/transformers\n",
      "Obtaining file:///content/transformers\n",
      "  Installing build dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Checking if build backend supports build_editable ... \u001B[?25l\u001B[?25hdone\n",
      "  Getting requirements to build editable ... \u001B[?25l\u001B[?25hdone\n",
      "  Preparing editable metadata (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0.dev0) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0.dev0) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0.dev0) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0.dev0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0.dev0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0.dev0) (2023.6.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0.dev0) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.32.0.dev0)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.8/7.8 MB\u001B[0m \u001B[31m46.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0.dev0) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0.dev0) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.32.0.dev0) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.32.0.dev0) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.32.0.dev0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.32.0.dev0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.32.0.dev0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.32.0.dev0) (2023.11.17)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building editable for transformers (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for transformers: filename=transformers-4.32.0.dev0-0.editable-py3-none-any.whl size=38186 sha256=e7e9e4fa9da23562ff3e9c52642bcd06d94bca5d87b5df8c0222870f11716f57\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-1mv_hmy6/wheels/7c/35/80/e946b22a081210c6642e607ed65b2a5b9a4d9259695ee2caf5\n",
      "Successfully built transformers\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.0\n",
      "    Uninstalling tokenizers-0.15.0:\n",
      "      Successfully uninstalled tokenizers-0.15.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.35.2\n",
      "    Uninstalling transformers-4.35.2:\n",
      "      Successfully uninstalled transformers-4.35.2\n",
      "Successfully installed tokenizers-0.13.3 transformers-4.32.0.dev0\n"
     ]
    }
   ],
   "source": [
    "!git clone --single-branch --branch fast_tokenizers_BARTpho_PhoBERT_BERTweet https://github.com/datquocnguyen/transformers.git\n",
    "%cd transformers\n",
    "!pip3 install -e ."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "pip install pytorch-crf"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pQSrW--_mqaD",
    "outputId": "241e3e7c-99ff-4025-e827-e0c9166d2087"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting pytorch-crf\n",
      "  Downloading pytorch_crf-0.7.2-py3-none-any.whl (9.5 kB)\n",
      "Installing collected packages: pytorch-crf\n",
      "Successfully installed pytorch-crf-0.7.2\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Setting for Training, Fine-tuning and Evaluation.**\n"
   ],
   "metadata": {
    "id": "5VJdExEioSlL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, RobertaForTokenClassification, RobertaConfig, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from seqeval.metrics import f1_score, classification_report\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Define label map\n",
    "label_map = {'O': 0, 'B-Skill': 1, 'I-Skill': 2}\n",
    "\n",
    "# Save model for further use\n",
    "def save_model(model_state_dict, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save(model_state_dict, path)\n",
    "\n",
    "# Load the saved model\n",
    "def load_model(model_class, path, pretrained_model_name, num_labels):\n",
    "    model = model_class.from_pretrained(pretrained_model_name, num_labels=num_labels)\n",
    "    model.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Flatten labels for class weight computation\n",
    "def flatten_labels(data):\n",
    "    flat_labels = []\n",
    "    for _, _, labels in data:\n",
    "        flat_labels.extend([label for label in labels.numpy() if label != -100])\n",
    "    return flat_labels\n",
    "\n",
    "# Compute class weights\n",
    "def compute_class_weights(train_data, device):\n",
    "    flat_train_labels = flatten_labels(train_data)\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(flat_train_labels), y=flat_train_labels)\n",
    "    return torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Encode tokens and map labels to token ids\n",
    "\n",
    "def encode_labels(tokens, text_labels,tokenizer):\n",
    "    if not all(isinstance(token, str) for token in tokens):\n",
    "        tokens = [str(token) for token in tokens]\n",
    "\n",
    "    # max_length is determined by the same way on BERT models\n",
    "    encoded_inputs = tokenizer(tokens, is_split_into_words=True, add_special_tokens=True,\n",
    "                               max_length=32, truncation=True, padding='max_length',\n",
    "                               return_attention_mask=True, return_tensors='pt')\n",
    "\n",
    "    labels = []\n",
    "    attention_masks = []\n",
    "    previous_word_idx = None\n",
    "    is_first_token = True\n",
    "\n",
    "    for i, word_id in enumerate(encoded_inputs.word_ids()):\n",
    "        if word_id is None:\n",
    "            labels.append(label_map['O'])\n",
    "            if is_first_token:\n",
    "                attention_masks.append(1)\n",
    "            else:\n",
    "                attention_masks.append(0)\n",
    "        else:\n",
    "            labels.append(text_labels[word_id])\n",
    "            attention_masks.append(1)\n",
    "            is_first_token = False\n",
    "\n",
    "    labels = labels[:32] + [label_map['O']] * (32- len(labels))\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return encoded_inputs['input_ids'][0], attention_masks, labels\n",
    "\n",
    "# processes the data into a format suitable for training, including tokenization and label encoding.\n",
    "def process_dataframe(df, tokenizer, label_map):\n",
    "    grouped_data = df.groupby('sentence_id').agg({\n",
    "        'word': list,\n",
    "        'tag': list\n",
    "    }).reset_index()\n",
    "\n",
    "    new_rows = []\n",
    "\n",
    "    # Adding \"_\" underscore to help the model better understand\n",
    "    # because the tokenizer employed in PhoBERT use underscore character as the connection between syllables within a compound word\n",
    "    for _, row in grouped_data.iterrows():\n",
    "        new_tokens, new_labels = [], []\n",
    "        for token, label in zip(row['word'], row['tag']):\n",
    "            combined_token = str(token).replace(' ', '_')\n",
    "            new_tokens.append(combined_token)\n",
    "            new_labels.append(label)\n",
    "\n",
    "        new_rows.append({'word': new_tokens, 'tag': new_labels})\n",
    "\n",
    "    new_grouped_data = pd.DataFrame(new_rows)\n",
    "\n",
    "    new_grouped_data['label_ids'] = new_grouped_data['tag'].apply(lambda tags: [label_map[tag] for tag in tags])\n",
    "\n",
    "    encoded_data = [encode_labels(sentence_tokens, sentence_labels, tokenizer)\n",
    "                    for sentence_tokens, sentence_labels in zip(new_grouped_data['word'], new_grouped_data['label_ids'])]\n",
    "\n",
    "    input_ids, attention_masks, labels = zip(*encoded_data)\n",
    "\n",
    "    return torch.stack(input_ids), torch.stack(attention_masks), torch.tensor(labels)\n",
    "\n",
    "# Train a model\n",
    "def train_model(model, tokenizer, train_dataloader, dev_dataloader, optimizer, scheduler, device, num_epochs, class_weights):\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=\"Training\"):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "            model.zero_grad()\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            # Employing CrossEntropyLoss to calculate thee class weight\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "            loss = loss_fct(outputs.logits.view(-1, model.num_labels), b_labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f\"Epoch {epoch_i + 1} - Average training loss: {avg_train_loss}\")\n",
    "\n",
    "        dev_f1 = calculate_f1_score(model, dev_dataloader, tokenizer, device)\n",
    "        print(f\"Epoch {epoch_i + 1} - Dev F1 Score: {dev_f1}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Evaluate a model\n",
    "def evaluate_model(model, dataloader, tokenizer, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    all_predictions, all_true_labels, all_words = [], [], []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "            logits = outputs.logits\n",
    "            batch_predictions = torch.argmax(logits, dim=-1)\n",
    "            batch_predictions = batch_predictions.detach().cpu().numpy()\n",
    "\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        words = [tokenizer.convert_ids_to_tokens(input_id) for input_id in b_input_ids.to('cpu').numpy()]\n",
    "\n",
    "        for i in range(label_ids.shape[0]):\n",
    "            input_len = sum(b_input_mask[i])\n",
    "            sentence_predictions = [list(label_map.keys())[list(label_map.values()).index(label_idx)] for label_idx in batch_predictions[i][1:input_len-1]]\n",
    "            sentence_true_labels = [list(label_map.keys())[list(label_map.values()).index(l)] for l in label_ids[i][1:input_len-1]]\n",
    "            sentence_words = words[i][1:input_len-1]\n",
    "\n",
    "            all_predictions.append(sentence_predictions)\n",
    "            all_true_labels.append(sentence_true_labels)\n",
    "            all_words.extend(sentence_words)\n",
    "\n",
    "    return all_predictions, all_true_labels, all_words\n",
    "\n",
    "\n",
    "# Calcute the F1 score\n",
    "def calculate_f1_score(model, dataloader, tokenizer, device):\n",
    "    predictions, true_labels,_ = evaluate_model(model, dataloader, tokenizer, device)\n",
    "    return f1_score(true_labels, predictions)\n",
    "\n",
    "# Save data with predicted labels for further use\n",
    "def save_predictions_to_csv(sentence_id, words, true_labels, predictions, file_path):\n",
    "    df = pd.DataFrame({\n",
    "        'Sentence_id': sentence_id,\n",
    "        'Word': words,\n",
    "        'True_Label': true_labels,\n",
    "        'Prediction': predictions\n",
    "    })\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Load and preprocess data\n",
    "    train_file_path = 'processed_kaggle_dataset.csv'\n",
    "    validation_file_path = 'processed_website_dataset.csv'\n",
    "\n",
    "    train_set = pd.read_csv(train_file_path)\n",
    "    validation_data_df = pd.read_csv(validation_file_path)\n",
    "\n",
    "    unique_sentence_ids = validation_data_df['sentence_id'].unique()\n",
    "    split_index = len(unique_sentence_ids) // 2\n",
    "    test_ids, dev_ids = unique_sentence_ids[:split_index], unique_sentence_ids[split_index:]\n",
    "    test_set = validation_data_df[validation_data_df['sentence_id'].isin(test_ids)]\n",
    "    dev_set = validation_data_df[validation_data_df['sentence_id'].isin(dev_ids)]\n",
    "\n",
    "    pretrained_model_name = 'vinai/phobert-base-v2'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_inputs, train_masks, train_labels = process_dataframe(train_set, tokenizer, label_map)\n",
    "    dev_inputs, dev_masks, dev_labels = process_dataframe(dev_set, tokenizer, label_map)\n",
    "    test_inputs, test_masks, test_labels = process_dataframe(test_set, tokenizer, label_map)\n",
    "\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=8)\n",
    "\n",
    "    dev_dataset = TensorDataset(dev_inputs, dev_masks, dev_labels)\n",
    "    dev_dataloader = DataLoader(dev_dataset, sampler=SequentialSampler(dev_dataset), batch_size=8)\n",
    "\n",
    "    test_dataset = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "    test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=8)\n",
    "\n",
    "    # Compute class weight\n",
    "    flat_train_labels = flatten_labels(train_dataset)\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(flat_train_labels), y=flat_train_labels)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "    # Training, Fine-tuning and evaluating process\n",
    "    learning_rates = [1e-3,1e-4, 1e-5]\n",
    "    num_epochs = 10\n",
    "    epsilon=1e-8\n",
    "    best_lr = None\n",
    "    best_f1_score = 0\n",
    "    best_model_info = {\n",
    "          \"f1_score\": 0,\n",
    "          \"learning_rate\": None,\n",
    "          \"epoch\": None,\n",
    "          \"weight_decay\": 0.01,\n",
    "          \"model_state_dict\": None\n",
    "      }\n",
    "\n",
    "    for lr in learning_rates:\n",
    "        print(f\"\\nTesting with learning rate: {lr}\")\n",
    "        model = RobertaForTokenClassification.from_pretrained(\n",
    "          pretrained_model_name,\n",
    "          num_labels=len(label_map)\n",
    "        )\n",
    "        model.to(device)\n",
    "        # Initialize the model and optimizer with the current learning rate\n",
    "        optimizer = AdamW(model.parameters(), lr=lr, eps=epsilon, weight_decay=0.01)\n",
    "        total_steps = len(train_dataloader) * num_epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "        # Train the model and capture training performance\n",
    "        model = train_model(model, tokenizer, train_dataloader, dev_dataloader, optimizer, scheduler, device, num_epochs, class_weights)\n",
    "        # Calculate F1-Score of the model with current learning rate on the development set\n",
    "        current_f1_score = calculate_f1_score(model, dev_dataloader, tokenizer, device)\n",
    "        print(f\"Learning rate: {lr} - F1 Score: {current_f1_score}\")\n",
    "\n",
    "        # Best learning rate is chosen by F1-Score\n",
    "        if current_f1_score > best_model_info[\"f1_score\"]:\n",
    "              best_model_info = {\n",
    "                  \"f1_score\": current_f1_score,\n",
    "                  \"learning_rate\": lr,\n",
    "                  \"model_state_dict\": model.state_dict()\n",
    "            }\n",
    "\n",
    "    # Save the best model\n",
    "    best_model_save_path = 'phobert_best_model.pth'\n",
    "    save_model(best_model_info[\"model_state_dict\"], best_model_save_path)\n",
    "    print(f\"\\nBest model saved at: {best_model_save_path}\")\n",
    "\n",
    "    # Load the best model for evaluation\n",
    "    best_model = load_model(RobertaForTokenClassification, best_model_save_path, pretrained_model_name, len(label_map))\n",
    "    best_model.to(device)\n",
    "\n",
    "    # Evaluate the best model on the dev set\n",
    "    dev_predictions, dev_true_labels,_ = evaluate_model(best_model, dev_dataloader, tokenizer, device)\n",
    "    dev_report = classification_report(dev_true_labels, dev_predictions)\n",
    "    print(f\"\\nDevelopment Set Classification Report:\\n{dev_report}\")\n",
    "\n",
    "    # Evaluate the best model on the test set\n",
    "    test_predictions, test_true_labels, test_words = evaluate_model(best_model, test_dataloader, tokenizer, device)\n",
    "    test_report = classification_report(test_true_labels, test_predictions)\n",
    "    print(f\"\\nTest Set Classification Report:\\n{test_report}\")\n",
    "\n",
    "    # Save output\n",
    "    gold_labels, predicted_labels, sentence_ids = [], [], []\n",
    "    for sentence_id, sentence_labels in enumerate(test_true_labels):\n",
    "        sentence_length = len(sentence_labels)\n",
    "        gold_labels.extend(sentence_labels)\n",
    "        predicted_labels.extend(test_predictions[sentence_id])\n",
    "        sentence_ids.extend([sentence_id] * sentence_length)\n",
    "    output_csv_path = f'phobert_best_model_output.csv'\n",
    "    save_predictions_to_csv(sentence_ids, test_words, gold_labels, predicted_labels, output_csv_path)\n",
    "    print(f\"Test predictions saved to {output_csv_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x0BD9AGJYqqG",
    "outputId": "0550b4ac-a4b5-4a12-b21e-9b1101ed2d66"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Testing with learning rate: 0.001\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/content/transformers/src/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Training: 100%|██████████| 125/125 [00:11<00:00, 10.63it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 - Average training loss: 1.1271787385940553\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:00<00:00, 57.15it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 - Dev F1 Score: 0.041312272174969626\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 125/125 [00:11<00:00, 10.79it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 2 - Average training loss: 1.106575803756714\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:01<00:00, 39.87it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 2 - Dev F1 Score: 0.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 125/125 [00:11<00:00, 10.81it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 3 - Average training loss: 1.1037739925384522\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:01<00:00, 41.02it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 3 - Dev F1 Score: 0.053450164293537786\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 125/125 [00:12<00:00, 10.17it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 4 - Average training loss: 1.1028475313186645\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:00<00:00, 54.88it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 4 - Dev F1 Score: 0.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 125/125 [00:11<00:00, 10.57it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5 - Average training loss: 1.1020944147109986\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:00<00:00, 55.93it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5 - Dev F1 Score: 0.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 125/125 [00:11<00:00, 10.59it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 6 - Average training loss: 1.1034166316986085\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:00<00:00, 55.38it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 6 - Dev F1 Score: 0.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 125/125 [00:11<00:00, 10.58it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 7 - Average training loss: 1.100015296936035\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:01<00:00, 41.37it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 7 - Dev F1 Score: 0.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 125/125 [00:11<00:00, 10.78it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 8 - Average training loss: 1.0997870063781738\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:01<00:00, 41.84it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 8 - Dev F1 Score: 0.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 125/125 [00:11<00:00, 10.64it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 9 - Average training loss: 1.0992054748535156\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:00<00:00, 55.93it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 9 - Dev F1 Score: 0.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 125/125 [00:11<00:00, 10.49it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 10 - Average training loss: 1.0978698205947877\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:00<00:00, 56.95it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 10 - Dev F1 Score: 0.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:00<00:00, 57.14it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Learning rate: 0.001 - F1 Score: 0.0\n",
      "\n",
      "Testing with learning rate: 0.0001\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training: 100%|██████████| 125/125 [00:11<00:00, 10.53it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 - Average training loss: 0.4337220377922058\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:00<00:00, 54.88it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 - Dev F1 Score: 0.35463546354635467\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 125/125 [00:11<00:00, 10.51it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 2 - Average training loss: 0.2956003125011921\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:00<00:00, 55.49it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 2 - Dev F1 Score: 0.2518628912071535\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 125/125 [00:11<00:00, 10.60it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 3 - Average training loss: 0.23372454723715783\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:01<00:00, 41.64it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 3 - Dev F1 Score: 0.40379146919431286\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 125/125 [00:11<00:00, 10.66it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 4 - Average training loss: 0.1514118373245001\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:01<00:00, 41.37it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 4 - Dev F1 Score: 0.47597254004576656\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 125/125 [00:11<00:00, 10.63it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5 - Average training loss: 0.12002944368869066\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:00<00:00, 55.16it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5 - Dev F1 Score: 0.5182863113897597\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 125/125 [00:11<00:00, 10.45it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 6 - Average training loss: 0.08266250353306531\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:00<00:00, 54.15it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 6 - Dev F1 Score: 0.5135135135135135\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 125/125 [00:11<00:00, 10.45it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 7 - Average training loss: 0.05587905881926417\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:00<00:00, 55.47it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 7 - Dev F1 Score: 0.490134994807892\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 125/125 [00:11<00:00, 10.42it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 8 - Average training loss: 0.03470095931366086\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:01<00:00, 47.66it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 8 - Dev F1 Score: 0.5432098765432098\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 125/125 [00:11<00:00, 10.49it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 9 - Average training loss: 0.03207064516469836\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:01<00:00, 40.92it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 9 - Dev F1 Score: 0.5384615384615384\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 125/125 [00:11<00:00, 10.58it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 10 - Average training loss: 0.02457548227161169\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:01<00:00, 41.80it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 10 - Dev F1 Score: 0.5352422907488987\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:01<00:00, 40.64it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Learning rate: 0.0001 - F1 Score: 0.5352422907488987\n",
      "\n",
      "Testing with learning rate: 1e-05\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training: 100%|██████████| 125/125 [00:11<00:00, 10.48it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 - Average training loss: 0.6667037689685822\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:01<00:00, 41.20it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 - Dev F1 Score: 0.2788244159758855\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 125/125 [00:12<00:00, 10.41it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 2 - Average training loss: 0.31588127100467683\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:00<00:00, 53.05it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 2 - Dev F1 Score: 0.3290267011197244\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 125/125 [00:12<00:00, 10.29it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 3 - Average training loss: 0.2679609753489494\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:00<00:00, 53.84it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 3 - Dev F1 Score: 0.3709981167608286\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 125/125 [00:12<00:00, 10.29it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 4 - Average training loss: 0.2154887993633747\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:00<00:00, 54.03it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 4 - Dev F1 Score: 0.4058536585365854\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 125/125 [00:12<00:00, 10.31it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5 - Average training loss: 0.1902212354838848\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:01<00:00, 42.45it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5 - Dev F1 Score: 0.45201238390092885\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 125/125 [00:11<00:00, 10.44it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 6 - Average training loss: 0.1764991851449013\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:01<00:00, 40.47it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 6 - Dev F1 Score: 0.44373673036093414\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 125/125 [00:11<00:00, 10.50it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 7 - Average training loss: 0.14478787258267403\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:01<00:00, 44.69it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 7 - Dev F1 Score: 0.45286885245901637\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 125/125 [00:12<00:00, 10.33it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 8 - Average training loss: 0.13758896723389624\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:00<00:00, 53.65it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 8 - Dev F1 Score: 0.45603271983640087\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 125/125 [00:12<00:00, 10.36it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 9 - Average training loss: 0.12043536245822907\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:00<00:00, 54.16it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 9 - Dev F1 Score: 0.4783068783068783\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 125/125 [00:12<00:00, 10.33it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 10 - Average training loss: 0.1157145794481039\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:01<00:00, 47.21it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 10 - Dev F1 Score: 0.4807492195629553\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:00<00:00, 53.58it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Learning rate: 1e-05 - F1 Score: 0.4807492195629553\n",
      "\n",
      "Best model saved at: /content/drive/MyDrive/testBA/PhoBERT_model/best_model.pth\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating: 100%|██████████| 50/50 [00:01<00:00, 50.00it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Development Set Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Skill       0.51      0.57      0.54       428\n",
      "\n",
      "   micro avg       0.51      0.57      0.54       428\n",
      "   macro avg       0.51      0.57      0.54       428\n",
      "weighted avg       0.51      0.57      0.54       428\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating: 100%|██████████| 50/50 [00:00<00:00, 54.81it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Test Set Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Skill       0.57      0.58      0.57       540\n",
      "\n",
      "   micro avg       0.57      0.58      0.57       540\n",
      "   macro avg       0.57      0.58      0.57       540\n",
      "weighted avg       0.57      0.58      0.57       540\n",
      "\n",
      "Test predictions saved to /content/drive/MyDrive/testBA/PhoBERT_model/best_model_output.csv\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}